---
output:
  pdf_document: default
  html_document: default
---
# SPACE-TIME NBI DATA ANALYSIS

### Required Libraries
```{r libraries}
# libraries
library(ranger)
library(plyr)
library(dplyr)
```
### LOAD US HIGHWAY NBI DATA EXISTED FROM 1992 to 2021
```{r}
# Introduce the path of 30-year NBI historical data
dir <- 'C:/Users/fs0163/Documents/FaribaUNT/Summer2022/HistoricalNBIData/USHighwayBridges/'

# Read NBI historical data.
files <- list.files(dir,pattern = '*.txt', full.names = TRUE)


# Identify the name of columns in the NBI data to retained. Note that retaining irrelevant columns leads to filling the memory. 
names <- c("STATE_CODE_001","STRUCTURE_NUMBER_008","HIGHWAY_DISTRICT_002","COUNTY_CODE_003","FEATURES_DESC_006A", "LOCATION_009", "LAT_016","LONG_017", "YEAR_BUILT_027", "TRAFFIC_LANES_ON_028A" ,"ADT_029", "YEAR_ADT_030", "DESIGN_LOAD_031", "STRUCTURE_KIND_043A", "STRUCTURE_TYPE_043B", "APPR_KIND_044A", "APPR_TYPE_044B", "MAIN_UNIT_SPANS_045", "MAX_SPAN_LEN_MT_048", "STRUCTURE_LEN_MT_049",  "LEFT_CURB_MT_050A", "RIGHT_CURB_MT_050B", "DECK_WIDTH_MT_052","DECK_COND_058", "SUPERSTRUCTURE_COND_059", "SUBSTRUCTURE_COND_060", "OPERATING_RATING_064", "DECK_GEOMETRY_EVAL_068", "DATE_OF_INSPECT_090", "YEAR_RECONSTRUCTED_106", "DECK_STRUCTURE_TYPE_107", "SURFACE_TYPE_108A","PERCENT_ADT_TRUCK_109", "FUTURE_ADT_114","YEAR_OF_FUTURE_ADT_115")
```
#### 1- Modeler enters startYear and endYear
```{r}
startYear <- 2016
endYear <- 2020
```
#### 2- Collecting historical NBI data between startYear and EndYear
```{r}

startYearID = as.integer(startYear) - 1991
endYearID = as.integer(endYear) - 1991

# Initialize an empty data frame.
USHWNBI <- data.frame()
for (i in startYearID:endYearID)
  {
  text_df <- read.delim2(files[i],sep = ",",quote="")
  USHWNBI<- rbind(text_df[,names],USHWNBI)
}

nrow(USHWNBI)    # There are 3,081,025 rows in  USHWNBI containing 5-year bridges from 2016 to 2020

```
### HISTORICAL NBI DATA PREPROCESSING

#### 1- Convert some variables into numeric
```{r}
# Convert several variables to numeric
cols.name <- c("LAT_016", "LONG_017", "ADT_029", "YEAR_ADT_030", "MAIN_UNIT_SPANS_045", "MAX_SPAN_LEN_MT_048", "STRUCTURE_LEN_MT_049", "LEFT_CURB_MT_050A", "RIGHT_CURB_MT_050B", "DECK_WIDTH_MT_052", "OPERATING_RATING_064", "DATE_OF_INSPECT_090", "YEAR_RECONSTRUCTED_106", "PERCENT_ADT_TRUCK_109", "FUTURE_ADT_114", "YEAR_OF_FUTURE_ADT_115")

# Convert some variables into numeric.
USHWNBI[cols.name] <- sapply(USHWNBI[cols.name], as.numeric)

```
#### 2- Remove Bridge Instances Containing Null Values
```{r}
# Remove any records containing null values.
USHWNBI <- na.omit(USHWNBI)
```
#### 3- Columns manipulation
##### - Add Latitude and Longitude
```{r}

# Convert coordinates stored in "LAT_016" and "LONG_017" of the NBI data into "Latitude" and "Longitude", respectively. This code is designed to convert either the correct and incorrect "LAT_016" and "LONG_017" into the correct form of "Latitude" and "Longitude", which have 2 positive integers and 3 negative integers, respectively.
USHWNBI$Latitude = (floor(USHWNBI$LAT_016/1000000)) + (floor((USHWNBI$LAT_016/1000000-(floor(USHWNBI$LAT_016/1000000)))*100)/60) + ((USHWNBI$LAT_016/10000-floor(USHWNBI$LAT_016/10000))*100/3600)

USHWNBI$Longitude <- ifelse (USHWNBI$LONG_017>0, -(floor(USHWNBI$LONG_017/1000000) + floor((USHWNBI$LONG_017/1000000-floor(USHWNBI$LONG_017/1000000))*100)/60 + (USHWNBI$LONG_017/10000- floor(USHWNBI$LONG_017/10000))*100/3600),-(floor(-USHWNBI$LONG_017/10000) + floor((-USHWNBI$LONG_017/10000-floor(-USHWNBI$LONG_017/10000))*100)/60 + (-USHWNBI$LONG_017/100-floor(-USHWNBI$LONG_017/100))*100/3600))

```
##### - Add Reconstructed Column
```{r}
# Create Reconstructed column indicating that a bridge is reconstructed or not.
USHWNBI$Reconstructed <- ifelse(USHWNBI$YEAR_RECONSTRUCTED_106 == 0,0,1)

```
##### - Add Inspection Year Column
```{r}
# Compute the year that the bridge was inspected. Derive the 2-digit number that reflects the year a bridge was inspected and term it as Inspection_Date_Year.
Inspection_Date_Year <- ((USHWNBI$DATE_OF_INSPECT_090/100)-floor(USHWNBI$DATE_OF_INSPECT_090 /100))*100

# Convert that 2-digit number into 4-digit number as the year of inspection in USHWNBI. 
USHWNBI$Inspection_Year <- ifelse(Inspection_Date_Year<40,2000+Inspection_Date_Year,1900+Inspection_Date_Year)

```
##### - Add Age Column

```{r}
# Compute the age of bridges.
USHWNBI$Age <- ifelse(USHWNBI$Reconstructed == 0, USHWNBI$Inspection_Year - USHWNBI$YEAR_BUILT_027 , USHWNBI$Inspection_Year - USHWNBI$YEAR_RECONSTRUCTED_106)

```
##### - Add Deck Area Column

```{r}
# Create "Deck_Area_" column and compute its values. 
USHWNBI$Deck_Area <- USHWNBI$STRUCTURE_LEN_MT_049 *USHWNBI$DECK_WIDTH_MT_052

```
##### - Add ADT based on Future_ADT and Year_Future_ADT

```{r}

USHWNBI$Diff_Year_ADT <-  USHWNBI$YEAR_OF_FUTURE_ADT_115- USHWNBI$YEAR_ADT_030
USHWNBI$Diff_ADT <- USHWNBI$FUTURE_ADT_114 - USHWNBI$ADT_029

USHWNBI$Diff_Year_ADT <- ifelse(USHWNBI$Diff_Year_ADT==0,NA,USHWNBI$Diff_Year_ADT)

USHWNBI$Diff_ADT_Div_Dif_Year <- USHWNBI$Diff_ADT/USHWNBI$Diff_Year_ADT

USHWNBI$ADT <- ifelse(is.na(USHWNBI$Diff_ADT_Div_Dif_Year),USHWNBI$ADT_029,USHWNBI$Diff_ADT_Div_Dif_Year*(USHWNBI$Inspection_Year - USHWNBI$YEAR_ADT_030) + USHWNBI$ADT_029)

```
##### - Add Curb_Width

```{r}
# Calculate Curb_width using left and right curb widths.
USHWNBI$Curb_Width <- USHWNBI$LEFT_CURB_MT_050A + USHWNBI$RIGHT_CURB_MT_050B

```
##### - Add NOAA_Climate_Regions

```{r}
library(dplyr)

# Convert states into NOAA climatically consistent regions. Note that there might be few records that their states are not correct. 

USHWNBI$NOAA_Climate_Regions <- plyr::mapvalues(USHWNBI$STATE_CODE_001,from= c(10, 24, 34, 42, 9, 44, 23, 25, 33, 36, 50, 11, 49, 8, 35, 4, 27, 55, 26, 19, 17, 18, 39, 54, 21, 47, 29, 20, 40, 48, 5, 22, 28, 30, 56, 38, 46, 31, 51, 37, 45, 13, 12, 1, 16, 53, 41, 6, 32, 2, 15, 72), to = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 9, 9, 10, 11, 12))

```
##### - Rename Several Columns

```{r}
# Initial column names
names <- c("STATE_CODE_001","STRUCTURE_NUMBER_008","HIGHWAY_DISTRICT_002","COUNTY_CODE_003","FEATURES_DESC_006A", "LOCATION_009", "LAT_016","LONG_017", "YEAR_BUILT_027", "TRAFFIC_LANES_ON_028A" ,"ADT_029", "YEAR_ADT_030", "DESIGN_LOAD_031", "STRUCTURE_KIND_043A", "STRUCTURE_TYPE_043B", "APPR_KIND_044A", "APPR_TYPE_044B", "MAIN_UNIT_SPANS_045", "MAX_SPAN_LEN_MT_048", "STRUCTURE_LEN_MT_049",  "LEFT_CURB_MT_050A", "RIGHT_CURB_MT_050B", "DECK_WIDTH_MT_052","DECK_COND_058", "SUPERSTRUCTURE_COND_059", "SUBSTRUCTURE_COND_060", "OPERATING_RATING_064", "DECK_GEOMETRY_EVAL_068", "DATE_OF_INSPECT_090", "YEAR_RECONSTRUCTED_106", "DECK_STRUCTURE_TYPE_107", "SURFACE_TYPE_108A","PERCENT_ADT_TRUCK_109", "FUTURE_ADT_114","YEAR_OF_FUTURE_ADT_115")

# Rename several columns
USHWNBI$Structure_Number <- USHWNBI$STRUCTURE_NUMBER_008   # Keep Structure_Number
USHWNBI$County_Code <- USHWNBI$COUNTY_CODE_003             # Keep County_Code
USHWNBI$Features_Description <- USHWNBI$FEATURES_DESC_006A # Keep Features_Description
USHWNBI$Location <- USHWNBI$LOCATION_009                   # Keep Location
USHWNBI$Lanes_On <- USHWNBI$TRAFFIC_LANES_ON_028A
USHWNBI$Number_Spans_Main <- USHWNBI$MAIN_UNIT_SPANS_045
USHWNBI$Length_Max_Span <- USHWNBI$MAX_SPAN_LEN_MT_048
USHWNBI$Operating_Rating <- USHWNBI$OPERATING_RATING_064
USHWNBI$ADTT <- as.numeric(USHWNBI$PERCENT_ADT_TRUCK_109)

```
##### - Introduce Categorical Variables as Factors

```{r}

# Convert several categorical variables as factors and rename them
USHWNBI$Highway_District <- as.factor(USHWNBI$HIGHWAY_DISTRICT_002)
USHWNBI$Design_Load <- as.factor(USHWNBI$DESIGN_LOAD_031)
USHWNBI$Reconstructed <- as.factor(USHWNBI$Reconstructed)
USHWNBI$Main_Material <- as.factor(USHWNBI$STRUCTURE_KIND_043A)
USHWNBI$Main_Design <- as.factor(USHWNBI$STRUCTURE_TYPE_043B)
USHWNBI$Spans_Material <- as.factor(USHWNBI$APPR_KIND_044A)
USHWNBI$Spans_Design <- as.factor(USHWNBI$APPR_TYPE_044B)
USHWNBI$Deck_Geometry <- as.factor(USHWNBI$DECK_GEOMETRY_EVAL_068)
USHWNBI$Deck_Type <- as.factor(USHWNBI$DECK_STRUCTURE_TYPE_107)
USHWNBI$Wearing_Surface <- as.factor(USHWNBI$SURFACE_TYPE_108A)
USHWNBI$NOAA_Climate_Regions <- as.factor(USHWNBI$NOAA_Climate_Regions)
USHWNBI$Deck_Condition <- as.factor(USHWNBI$DECK_COND_058)
USHWNBI$Superstructure_Condition <- as.factor(USHWNBI$SUPERSTRUCTURE_COND_059)
USHWNBI$Substructure_Condition <- as.factor(USHWNBI$SUBSTRUCTURE_COND_060)


```
##### - Retain and order columns

```{r}
# Retain some columns and order them. 
# Use it for classification
USHWNBI <- USHWNBI[ , c("Deck_Condition",  
                    "Age",
                    "ADT",
                    "ADTT",
                    "Lanes_On",
                    "Number_Spans_Main",                   
                    "Length_Max_Span",  
                    "Curb_Width",
                    "Deck_Area",     
                    "Operating_Rating",                    
                    "Highway_District",
                    "Design_Load",
                    "Reconstructed",
                    "Main_Material",
                    "Main_Design",
                    "Spans_Material",
                    "Spans_Design",                    
                    "Deck_Geometry",
                    "Deck_Type",
                    "Wearing_Surface",
                    "NOAA_Climate_Regions")] 
```
#  DATA PREPROCESSING
##### - Remove bridges with N deck_conditions

```{r}
USHWNBI <- USHWNBI[USHWNBI$Deck_Condition == 0 |USHWNBI$Deck_Condition == 1|USHWNBI$Deck_Condition == 2|USHWNBI$Deck_Condition == 3|USHWNBI$Deck_Condition == 4|USHWNBI$Deck_Condition == 5|USHWNBI$Deck_Condition == 6|USHWNBI$Deck_Condition == 7|USHWNBI$Deck_Condition == 8|USHWNBI$Deck_Condition == 9,]


# Drop levels after removing values
USHWNBI$Deck_Condition <- droplevels(USHWNBI$Deck_Condition)

```
##### - Remove bridges with null values

```{r}

USHWNBI <- USHWNBI[complete.cases(USHWNBI),]

```
##### - Remove bridges with negative values

```{r}
# Remove rows containing negative values in the following columns
USHWNBI <- USHWNBI[!USHWNBI$Age < 0 &
                     !USHWNBI$ADT < 0 &
                     !USHWNBI$ADTT < 0 &
                     !USHWNBI$Lanes_On < 0 &
                     !USHWNBI$Number_Spans_Main < 0 &
                     !USHWNBI$Length_Max_Span < 0 &
                     !USHWNBI$Curb_Width < 0 &
                     !USHWNBI$Deck_Area < 0 & 
                     !USHWNBI$Operating_Rating < 0,]

```
##### - Remove duplicate bridges

```{r}
# Remove duplicates 
USHWNBI<- USHWNBI[!duplicated(USHWNBI), ]

```
##### ---- Make a copy from USHWNBI

```{r}
US <- USHWNBI

```
##### - one-hot encoding categorical variables
```{r}
# transfer categorical variables to numeric
US$Highway_District <- as.numeric(mapvalues(US$Highway_District,from = c("2",  "3",  "7",  "62", "58", "93", "96", "64", "52", "26", "63", "32", "6",  "0",  "8",  "83", "82", "4",  "43", "15", "1",  "5",  "86", "53", "51", "87", "10", "73" ,"9" , "71", "61", "78", "34", "44", "85", "12", "91", "35" ,"14", "11" ,"23", "75", "33", "54", "16", "81" ,"25", "47",
 "84" ,"24", "41", "95" ,"94", "17", "42", "72", "92", "46", "13" ,"36", "97", "31", "18", "45", "74", "22", "21", "66", "20", "65" ,"19", "60", "57", "67", "68", "30", "2B", "70", "GL" ,"2C", "88", "27" ,"PR" ,"55", "IL" ,"NC", "80" ,"69", "OH", "KI", "TU" ,"40", "90", "PH", "SA", "48", "39", "HO", "37" ,"49", "PV", "KY"), to = c( 1,	2,	3,	4,	5,	6,	7,	8,	9,	10,	11,	12,	13,	14,	15,	16,	17,	18,	19,	20,	21,	22,	23,	24,	25,	26,	27, 28,	29,	30,	31,	32,	33,	34,	35,	36,	37,	38,	39,	40,	41,	42,	43,	44,	45,	46,	47,	48,	49,	50,	51,	52,	53,	54,	55,	56,	57,	58,	59,	60,	61,	62,	63,	64,	65,	66,	67,	68,	69,	70,	71,	72,	73,	74,	75,	76,	77,	78,	79,	80,	81,	82,	83,	84,	85,	86,	87,	88,	89,	90,	91,	92,	93,	94,	95,	96,	97,	98,	99,	100, 101,	102)))


US$Design_Load <- as.numeric(mapvalues(US$Design_Load, from =c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "A", "B", "C"),to =c(1,	2,	3,	4,	5,	6,	7,	8,	9,	10,	11,	12,	13)))

US$Reconstructed <- as.numeric(US$Reconstructed)

US$Main_Material <- as.numeric(mapvalues(US$Main_Material, from =c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9"),to =c(1,	2,	3,	4,	5,	6,	7,	8,	9,	10)))

US$Main_Design <- as.numeric(mapvalues(US$Main_Design, from =c("0",  "1",  "2", "3",  "4",  "5",  "6",  "7",  "8",  "9",  "10", "11", "12", "13", "14", "15", "16", "17", "19", "20", "21", "22"),to =c(1,	2,	3,	4,	5,	6,	7,	8,	9,	10,	11,	12,	13,	14,	15,	16,	17,	18,	19,	20,	21,	22)))

US$Spans_Material <- as.numeric(mapvalues(US$Spans_Material, from =c("0",  "1",  "2", "3",  "4",  "5",  "6",  "7",  "8",  "9"),to =c(1,	2,	3,	4,	5,	6,	7,	8,	9, 10)))

US$Spans_Design <- as.numeric(mapvalues(US$Spans_Design, from =c("0",  "1",  "2",  "3", "4",  "5", "6",  "7", "8",  "9",  "10", "11", "12", "13", "14", "15", "16", "17", "19", "20", "21", "22"),to =c(1,	2,	3,	4,	5,	6,	7,	8,	9,	10,	11,	12,	13,	14,	15,	16,	17,	18,	19,	20,	21,	22)))

US$Deck_Geometry <- as.numeric(mapvalues(US$Deck_Geometry, from =c("0", "2", "3", "4", "5", "6", "7", "8", "9", "N","*"),to =c(1,	2,	3,	4,	5,	6,	7,	8,	9,	10, 11)))

US$Deck_Type <- as.numeric(mapvalues(US$Deck_Type, from =c("1", "2", "3", "4", "5", "6", "7", "8", "9", "N"),to =c(1,	2,	3,	4,	5,	6,	7,	8,	9,	10)))

US$Wearing_Surface <- as.numeric(mapvalues(US$Wearing_Surface, from =c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "N"),to =c(1,	2,	3,	4,	5,	6,	7,	8,	9,	10, 11)))

US$NOAA_Climate_Regions <- as.numeric(US$NOAA_Climate_Regions)

```
##### - Satndardize variables
```{r}

# Standardize all variables
US$Age <- scale(US$Age)
US$ADT <- scale(US$ADT)
US$ADTT <- scale(US$ADTT)
US$Lanes_On <- scale(US$Lanes_On)
US$Number_Spans_Main <- scale(US$Number_Spans_Main)
US$Length_Max_Span <- scale(US$Length_Max_Span)
US$Curb_Width <- scale(US$Curb_Width)
US$Deck_Area <- scale(US$Deck_Area)
US$Operating_Rating <- scale(US$Operating_Rating)
US$Highway_District <- scale(US$Highway_District)
US$Design_Load <- scale(US$Design_Load)
US$Reconstructed <- scale(US$Reconstructed)
US$Main_Material <- scale(US$Main_Material)
US$Main_Design <- scale(US$Main_Design)
US$Spans_Material <- scale(US$Spans_Material)
US$Spans_Design <- scale(US$Spans_Design)
US$Deck_Geometry <- scale(US$Deck_Geometry)
US$Deck_Type <- scale(US$Deck_Type)
US$Wearing_Surface <- scale(US$Wearing_Surface)
US$NOAA_Climate_Regions <- scale(US$NOAA_Climate_Regions)

```
##### ---- Make a copy from US

```{r}
US_raw <- US
```
#  MULTICLASS CLASSIFICATION
## 1- Random forest
```{r}
US <- US_raw

# Remove structures without decks
US <- US[US$Deck_Condition == 0 |US$Deck_Condition == 1|US$Deck_Condition == 2|US$Deck_Condition == 3|US$Deck_Condition == 4|US$Deck_Condition == 5|US$Deck_Condition == 6|US$Deck_Condition == 7|US$Deck_Condition == 8|US$Deck_Condition == 9,]


# Drop levels after removing values
US$Deck_Condition <- droplevels(US$Deck_Condition)

# Partition data
set.seed(1234)
train.idx <- sample(nrow(US), 0.8*nrow(US))
train <- US[train.idx,]
test <- US[-train.idx,]


```
### -Tune hyperparameters (ntree)
```{r}
# When the OOB error rate becomes stabilize, it can be used as a good estimate for the generalization error.
nt_ntree <- seq(1,601, 200)
oob_ntree <- vector("numeric", length(nt_ntree))

for(i in 1:length(nt_ntree)){rf_multi <- ranger(Deck_Condition ~ .,
                     data = train,
                     num.trees = nt_ntree[i],
                     write.forest = FALSE,
                     seed =1234)}


# Plot the out-of-bag error over different number of trees. 
par(mfrow = c(1, 1))
par(mar=c(5,3,1,1)+5)   # extra large bottom margin
plot(x = nt_ntree, y = oob_ntree, col = "red", type = "l",xlab= "Number of trees", ylab= "Out-of-bag #error", cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5,lwd=3.0)

```
### -Tune hyperparameters (mtry)
```{r}
# When the OOB error rate becomes stabilize, it can be used as a good estimate for the generalization error.
nt_mtry <- seq(5,11,2)
oob_mtry <- vector("numeric", length(nt_mtry))

for(i in 1:length(nt_mtry)){
  rf_bin <- ranger(Deck_Condition ~ .,
                   data = train,
                   num.trees = 500,
                   mtry = nt_mtry[i],
                   write.forest = FALSE,
                   seed =1234)
oob_mtry[i] <- rf_bin$prediction.error
}

# Plot the out-of-bag error over different number of mtry. 
par(mfrow = c(1, 1))
par(mar=c(5,3,1,1)+5)   # extra large bottom margin
plot(x = nt_mtry, y = oob_mtry, col = "red", type = "l",xlab= "Number of mtry", ylab= "Out-of-bag #error", cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5,lwd=3.0)

```
### - Train
```{r}

# train random forest 
start.time_randomforest <- Sys.time()
rf_multi <- ranger(Deck_Condition ~ .,
                   data = train,
                   mtry = 5,
                   importance = "permutation",
                   num.trees =500,
                   seed =1234)
end.time_randomforest <- Sys.time()
end.time_randomforest-start.time_randomforest
```
### - Evaluation
```{r}
# predict the classes using the trained random forest
pred_rf <- predict(rf_multi, data = test)

# confusion matrix
library(caret)
confusionMatrix <- confusionMatrix(pred_rf$predictions, test$Deck_Condition)
confusionMatrix
```
### - Feature importance
```{r}
# Variable Importance plot
par(mfrow = c(1, 1))
par(mar=c(1,25,1,1)+4)   # extra large bottom margin
barplot(sort(rf_multi$variable.importance),horiz = TRUE,las=1,cex.names =1.5,cex.axis = 2,xlim = c(0, 0.25))

```
## 2- XGBoost
#### - Train
```{r}

# Remove structures without decks
US <- US[US$Deck_Condition == 0 |US$Deck_Condition == 1|US$Deck_Condition == 2|US$Deck_Condition == 3|US$Deck_Condition == 4|US$Deck_Condition == 5|US$Deck_Condition == 6|US$Deck_Condition == 7|US$Deck_Condition == 8|US$Deck_Condition == 9,]

# Drop levels after removing values
US$Deck_Condition <- droplevels(US$Deck_Condition)

unique(US$Deck_Condition)


# Packages
library(xgboost)
library(magrittr)
library(dplyr)
library(Matrix)


# Partition data
set.seed(1234)
train.idx <- sample(nrow(US), 0.8*nrow(US))
train <- US[train.idx,]
test <- US[-train.idx,]


# Create matrix - One-Hot Encoding for Factor variables
trainm <- sparse.model.matrix(Deck_Condition ~., data = train)
train_label <- as.numeric(train$Deck_Condition)-1
train_matrix <- xgb.DMatrix(data = as.matrix(trainm), label = train_label)

# Create matrix - One-Hot Encoding for Factor variables
testm <- sparse.model.matrix(Deck_Condition ~., data = test)
test_label <- as.numeric(test$Deck_Condition)-1
test_matrix <- xgb.DMatrix(data = as.matrix(testm), label = test_label)

# Parameters
nc <- length(unique(train_label))
xgb_params <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = nc)
watchlist <- list(train = train_matrix, test = test_matrix)


# eXtreme Gradient Boosting Model
start_time_xgboost <- Sys.time()
seed =1234
xgboost <- xgb.train(params = xgb_params,
                       data = train_matrix,
                       nrounds = 300,   # best round would be 40
                       watchlist = watchlist,
                       early_stopping_rounds = 3)
                       
end_time_xgboost <- Sys.time()
end_time_xgboost -start_time_xgboost

```
### - Evaluation
```{r}

# Explore the xgboost model
xgboost

# Training & test error plot
e <- data.frame(xgboost$evaluation_log)
plot(e$iter, e$train_mlogloss,col = 'blue')
lines(e$iter, e$test_mlogloss, col = 'red')

# Minimum error in test data
min(e$test_mlogloss)

# The iteration that experienced less error
e[e$test_mlogloss == min(e$test_mlogloss),]


# Prediction & Confusion Matrix - Test data
p<- predict(xgboost, newdata = test_matrix)
pred <- matrix(p, nrow = nc, ncol = length(p)/nc) %>%
  t() %>%
  data.frame() %>%
  mutate(label = test_label, max_prob = max.col(.,"last")-1)

table(Actual = pred$label ,Prediction = pred$max_prob)


```
### - Feature importance
```{r}

# Feature importance
imp <- xgb.importance(colnames(train_matrix), model = xgboost)
imp
xgb.plot.importance(imp)


```
## 3- Neural Network
#### - Data Preparation
```{r}
library(keras)

US <- US_raw

# Partitioning data
set.seed(1234)
indexes=caret::createDataPartition(US$Deck_Condition, p=.80, list = FALSE)
train_nn = US[indexes, ]
test_nn = US[-indexes, ] 

# Neural Network Visualization
library(neuralnet)
library(caret)

nnet <- neuralnet::neuralnet(Deck_Condition ~ Age+ ADT+ ADTT+ Lanes_On+ Number_Spans_Main+ Length_Max_Span+ Curb_Width+ Deck_Area+ Operating_Rating+ Highway_District+ Design_Load+ Reconstructed+ Main_Material+ Main_Design+ Spans_Material+ Spans_Design+ Deck_Geometry+ Deck_Type+ Wearing_Surface+ NOAA_Climate_Regions,
                 data = train_nn,
                 hidden = c(1,1),
                 linear.output = FALSE,
                 lifesign = 'full',
                 rep = 1)
  
plot(nnet,
     col.hidden = 'darkgreen',
     col.hidden.synapse = 'darkgreen',
     show.weights =  F,
     information = F,
     fill = 'lightblue')

```
## 4- Multilayer perceptron NN
#### - Train
https://www.youtube.com/watch?v=SrQw_fWo4lw&t=141s
```{r}
library(keras)

US <- US_raw

# Remove bridges with "N" condition rating
US <- US[US$Deck_Condition == "N",]
US <- US[US$Deck_Condition == 0 |US$Deck_Condition == 1|US$Deck_Condition == 2|US$Deck_Condition == 3|US$Deck_Condition == 4|US$Deck_Condition == 5|US$Deck_Condition == 6|US$Deck_Condition == 7|US$Deck_Condition == 8|US$Deck_Condition == 9,]

# Drop levels after removing values
US$Deck_Condition <- droplevels(US$Deck_Condition)
unique(US$Deck_Condition)

nrow(US)
ncol(US)

# Data Preparation
US[,1] <- as.numeric(US[,1])-1
US <- as.matrix(US)
dimnames(US) <- NULL
US[,2:21] <- keras::normalize(US[,2:21])

summary(US)

# Partition data
set.seed(1234)
train.idx <- sample(nrow(US), 0.8*nrow(US)) # Sampling is the same for all models
train <- US[train.idx, 2:21]
test <- US[-train.idx, 2:21]
traintarget <- US[train.idx,1]
testtarget <- US[-train.idx,1]
trainLabels <- to_categorical(traintarget)
testLabels <- to_categorical(testtarget)

# Specify the training control with 10-fold cross-validation
ctrl <- trainControl(method = "cv" , number = 10)


# Function to create the neural network model
create_model <- function(nodes, activation, dropout_rate){
  model <- keras_model_sequential()
  model %>%
    layer_dense(units = nodes[1], activation = activation[1], input_shape = ncol(train)) %>%
    layer_dropout(rate = dropout_rate[1])%>% 
    layer_dense(units = nodes[2], activation = activation[2]) %>%
    layer_dropout(rate = dropout_rate[2])%>% 
    ayer_dense(units = nodes[3], activation = activation[3], input_shape = ncol(train)) %>%
    layer_dropout(rate = dropout_rate[3])%>% 
    layer_dense(units = nodes[4], activation = activation[4]) %>%
    layer_dropout(rate = dropout_rate[4])%>% 
    layer_dense(units = nodes[5], activation = activation[5]) %>%
    layer_dropout(rate = dropout_rate[5])%>% 
    layer_dense(units = 10, activation = 'softmax')
  
  model%>%
     compile(loss = 'categorical_crossentropy',
             optimizer = 'adam',
             metrics = 'accuracy')
  
  return(model)
}


# Define the grid for hyperparameter search
grid <- expand.grid(
  nodes = c(32,64,128,256,512),
  activation = c("relu", "selu", "tanh"),
  dropout_rate = c(0.1, 0.1, 0.2, 0.3, 0.4, 0.5),
  epochs = c(50,100,150),
  batch_size = c(32,64)
  )



# Train the model with 10-fold cross validation
set.seed(1234)
start.time_cv <- Sys.time()
cv_model <- train(
  x = train,
  y = trainLabels,
  method = "keras",
  trControl = ctrl,
  metric = "Accuracy",
  tuneGrid = grid,
  tuneLength = 10, # number of combinations to try
  model = create_model
)
end.time_cv <- Sys.time()
 
# Print cross-validation results
print(cv_model)

# Calculate and print cross-validation time
cv_time <- end.time_cv - start.time_cv
print(cv_time)

# Predict class probabilities on the test set
test_predictions <- predict(cv_model, test)

# Convert class probabilities to predicted class labels
predicted_labels <- max.col(test_predictions) -1

# Convert true labels to numeric values
true_labels <- max.col(testLabels) -1

# Create confusion matrix
conf_matrix <- confusionMatrix(predicted_labels, true_labels)

# Print confusion matrix
print(conf_matrix)

```














